{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordEmbeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/turmovasiring/Notes/blob/master/WordEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KuFJ1qlS4Fa",
        "colab_type": "text"
      },
      "source": [
        "# Word embeddings\n",
        "\n",
        "Every word  can be represented by a set of real numbers (a vector). Word embeddings are N-dimensional vectors capable to capture the meaning of words and their relationship between words. \n",
        "\n",
        "Currently, word embeddings have broad applications in the area of natural language processing (NLP) because they allow us to implicitly include external information from the world into our language understanding models. Word embedding models have significantly improved the results in many NLP tasks such as text classification, NER, and so on. \n",
        "\n",
        "In a word embedding model, each word is represented by a unique word embedding (=vector), which captures its meaning. Similar words should have close word embeddings. A word embedding model usually shows amazing and useful properties. In particular, it is able to identify similar words and capture the relationships between words. Thus, similar words usually have close locations in the vectorial space of the model. In the picture above, you can see that the words “car”, “vehicle”, and “van” are close in the embedding space, far away from non-related words like “moon”, “space”, “tree” etc.\n",
        "\n",
        "<img src='https://shanelynnwebsite-mid9n9g1q9y8tt.netdna-ssl.com/wp-content/uploads/2018/01/word-vector-space-similar-words.png'/>\n",
        "\n",
        "\n",
        "To measure the similarity between two words, you can use the **cosine similarity** (the angle between two vectors in space) or the **euclidean distance** (distance between points).\n",
        "\n",
        "\n",
        "Word embeddings are very useful for machine learning algorithms, because the similarity between words allows us to work with words that do not exist in the training dataset. The ability to handle unseen terms is a huge advantage of word embedding approaches over the previous  TF-IDF / bag-of-words approaches.\n",
        "\n",
        "Moreover, word embeddings are also able to capture relationships between words (between its vectors). As you can see in the picture above, the transformation between the vector for “man” and “woman” is similar to the transformation between “king” and “queen”, “uncle” and “aunt”.\n",
        "\n",
        "<img src='https://vecto.space/assets/img/queen.png'/>\n",
        "\n",
        "Other interesting patterns have been documented in several papers about word embedding models (see references). These relationships are not explicitly defined in the training dataset, but are “discovered” from the use of language in the training texts.\n",
        "\n",
        "<img src='https://shanelynnwebsite-mid9n9g1q9y8tt.netdna-ssl.com/wp-content/uploads/2018/02/vocabulary-linear-relationships.png'/>\n",
        "\n",
        "\n",
        "## Approaches to train word embedding models\n",
        "\n",
        "\n",
        "There are two primary approaches to training word embedding models:\n",
        "\n",
        "- **Distributed Semantic Models**: These models use a co-occurance matrix built from a large text corpus. The matrix denotes the probability that words occur closely together. This matrix is factorised (using SVD / PCA / similar) to form a word vector matrix. \n",
        "\n",
        "- **Neural Network Models**: they are “predict approaches”, where models are constructed to predict the context words from a centre word (skip gram), or the centre word from a set of context words (CBOW). \n",
        "\n",
        "\n",
        "<img src='https://miro.medium.com/max/2800/0*o2FCVrLKtdcxPQqc.png'/>\n",
        "\n",
        "\n",
        "## Neural Network tools to train word embedding models.\n",
        "\n",
        "Neural Network models usually overcome semantic models. The most popular algorithms to train word embeddings are:\n",
        "\n",
        "1. **Word2Vec** , developed by Google. It uses CBOW and Skip-Gram techniques.  <a href='https://code.google.com/archive/p/word2vec/'>link</a>\n",
        "2. **Glove** “Global Vectors for Word Representations”  from Standford University.  <a href='https://nlp.stanford.edu/projects/glove/'>link</a>\n",
        "3. **Fasttext** created by Facebook. <a href='https://fasttext.cc/'>link</a>\n",
        "\n",
        "\n",
        "You can find more information about these models at:\n",
        "\n",
        "- Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). **Distributed representations of words and phrases and their compositionality**. In Advances in neural information processing systems (pp. 3111-3119). <a href='https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf'>link</a>\n",
        "\n",
        "- Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). **Efficient estimation of word representations in vector space**. <a href='https://arxiv.org/abs/1301.3781'>link</a>\n",
        "\n",
        "- Pennington, J., Socher, R., & Manning, C. D. (2014, October). **Glove: Global vectors for word representation**. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543). <a href='https://www.aclweb.org/anthology/D14-1162.pdf'>link</a>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}